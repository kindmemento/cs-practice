// Gradient Boosting builds an additive model by training weak learners (usually decision trees) sequentially.
// Each tree is trained to correct the errors of the combined ensemble.
// The algorithm minimizes a loss function, and each tree is added to the ensemble with a weight that is determined by the optimization process.
