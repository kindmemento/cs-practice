// Gradient Boosting builds an additive model by training weak learners (usually decision trees) sequentially.
// Each tree is trained to correct the errors of the combined ensemble.
// The algorithm minimizes a loss function, and each tree is added to the ensemble with a weight that is determined by the optimization process.

// Imagine you are working on a project to predict housing prices.
// Gradient Boosting can be applied to build an ensemble of decision trees, where each tree corrects the errors of the previous ones.
// This leads to a robust model capable of capturing complex relationships in the data and making accurate predictions.
